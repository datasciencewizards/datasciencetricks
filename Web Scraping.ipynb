{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "### Web scraping, also known as web data extraction, is the process of retrieving or “scraping” data from a website.Unlike the conventional process of manually extracting data, web scraping uses intelligent automation to retrieve hundreds, millions, or even billions of data points from the internet’s seemingly endless frontier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "#pip install urllib\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading URL into our notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Urllib module is the URL handling module for python. It is used to fetch URLs (Uniform Resource Locators). It uses the urlopen function and is able to fetch URLs using a variety of different protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "\n",
    "#Reading url into a variable using urllib\n",
    "html = urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We create a BeautifulSoup object by passing two arguments:\n",
    "###### html : It is the raw HTML content.\n",
    "###### lxml :Specifying the HTML parser we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating parse tree for the html page\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing soup functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Data science - Wikipedia</title>\n"
     ]
    }
   ],
   "source": [
    "# Get the title of the page\n",
    "title = soup.title\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### pretiffy() gives the visual representation of the parse tree created from the raw HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avoiding it here to eliminate longer prints \n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a id=\"top\"></a>,\n",
       " <a class=\"mw-jump-link\" href=\"#mw-head\">Jump to navigation</a>,\n",
       " <a class=\"mw-jump-link\" href=\"#p-search\">Jump to search</a>,\n",
       " <a href=\"/wiki/Information_science\" title=\"Information science\">information science</a>,\n",
       " <a href=\"/wiki/Machine_learning\" title=\"Machine learning\">Machine learning</a>,\n",
       " <a href=\"/wiki/Data_mining\" title=\"Data mining\">data mining</a>,\n",
       " <a class=\"image\" href=\"/wiki/File:Kernel_Machine.svg\"><img alt=\"Kernel Machine.svg\" data-file-height=\"233\" data-file-width=\"512\" decoding=\"async\" height=\"100\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x\" width=\"220\"/></a>,\n",
       " <a href=\"/wiki/Statistical_classification\" title=\"Statistical classification\">Classification</a>,\n",
       " <a href=\"/wiki/Cluster_analysis\" title=\"Cluster analysis\">Clustering</a>,\n",
       " <a href=\"/wiki/Regression_analysis\" title=\"Regression analysis\">Regression</a>,\n",
       " <a href=\"/wiki/Anomaly_detection\" title=\"Anomaly detection\">Anomaly detection</a>,\n",
       " <a href=\"/wiki/Automated_machine_learning\" title=\"Automated machine learning\">AutoML</a>,\n",
       " <a href=\"/wiki/Association_rule_learning\" title=\"Association rule learning\">Association rules</a>,\n",
       " <a href=\"/wiki/Reinforcement_learning\" title=\"Reinforcement learning\">Reinforcement learning</a>,\n",
       " <a href=\"/wiki/Structured_prediction\" title=\"Structured prediction\">Structured prediction</a>,\n",
       " <a href=\"/wiki/Feature_engineering\" title=\"Feature engineering\">Feature engineering</a>,\n",
       " <a href=\"/wiki/Feature_learning\" title=\"Feature learning\">Feature learning</a>,\n",
       " <a href=\"/wiki/Online_machine_learning\" title=\"Online machine learning\">Online learning</a>,\n",
       " <a href=\"/wiki/Semi-supervised_learning\" title=\"Semi-supervised learning\">Semi-supervised learning</a>,\n",
       " <a href=\"/wiki/Unsupervised_learning\" title=\"Unsupervised learning\">Unsupervised learning</a>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding all anchor tags and printing first 20\n",
    "soup.find_all('a')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "#mw-head\n",
      "#p-search\n",
      "/wiki/Information_science\n",
      "/wiki/Machine_learning\n",
      "/wiki/Data_mining\n",
      "/wiki/File:Kernel_Machine.svg\n",
      "/wiki/Statistical_classification\n",
      "/wiki/Cluster_analysis\n",
      "/wiki/Regression_analysis\n",
      "/wiki/Anomaly_detection\n",
      "/wiki/Automated_machine_learning\n",
      "/wiki/Association_rule_learning\n",
      "/wiki/Reinforcement_learning\n",
      "/wiki/Structured_prediction\n",
      "/wiki/Feature_engineering\n",
      "/wiki/Feature_learning\n",
      "/wiki/Online_machine_learning\n",
      "/wiki/Semi-supervised_learning\n",
      "/wiki/Unsupervised_learning\n"
     ]
    }
   ],
   "source": [
    "#Finding all outsource links\n",
    "all_links = soup.find_all(\"a\")\n",
    "for link in all_links[:20]:\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another way to read urls into our file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### When one makes a request to a URI, it returns a response. Python requests provides inbuilt functionalities for managing both the request and response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://web.mta.info/developers/data/nyct/turnstile/turnstile_200516.txt'\n",
    "\n",
    "#Command to use url in our script using requests\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We create a BeautifulSoup object by passing two arguments:\n",
    "###### resp.text : It is the raw HTML content.\n",
    "###### html.parser :Specifying the HTML parser we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the parse tree for new link\n",
    "soup_data = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating file handler to store data\n",
    "output = open('data.csv', 'wb' ) \n",
    "\n",
    "#writing contents of url to file\n",
    "output.write(resp.content) \n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading csv into the dataframe\n",
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C/A</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>SCP</th>\n",
       "      <th>STATION</th>\n",
       "      <th>LINENAME</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>DESC</th>\n",
       "      <th>ENTRIES</th>\n",
       "      <th>EXITS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>05/09/2020</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7417122</td>\n",
       "      <td>2518946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>05/09/2020</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7417123</td>\n",
       "      <td>2518948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>05/09/2020</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7417133</td>\n",
       "      <td>2518959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>05/09/2020</td>\n",
       "      <td>12:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7417146</td>\n",
       "      <td>2518977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>05/09/2020</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7417178</td>\n",
       "      <td>2518996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C/A  UNIT       SCP STATION LINENAME DIVISION        DATE      TIME  \\\n",
       "0  A002  R051  02-00-00   59 ST  NQR456W      BMT  05/09/2020  00:00:00   \n",
       "1  A002  R051  02-00-00   59 ST  NQR456W      BMT  05/09/2020  04:00:00   \n",
       "2  A002  R051  02-00-00   59 ST  NQR456W      BMT  05/09/2020  08:00:00   \n",
       "3  A002  R051  02-00-00   59 ST  NQR456W      BMT  05/09/2020  12:00:00   \n",
       "4  A002  R051  02-00-00   59 ST  NQR456W      BMT  05/09/2020  16:00:00   \n",
       "\n",
       "      DESC  ENTRIES  \\\n",
       "0  REGULAR  7417122   \n",
       "1  REGULAR  7417123   \n",
       "2  REGULAR  7417133   \n",
       "3  REGULAR  7417146   \n",
       "4  REGULAR  7417178   \n",
       "\n",
       "   EXITS                                                                 \n",
       "0                                            2518946                     \n",
       "1                                            2518948                     \n",
       "2                                            2518959                     \n",
       "3                                            2518977                     \n",
       "4                                            2518996                     "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 206905 entries, 0 to 206904\n",
      "Data columns (total 11 columns):\n",
      "C/A                                                                     206905 non-null object\n",
      "UNIT                                                                    206905 non-null object\n",
      "SCP                                                                     206905 non-null object\n",
      "STATION                                                                 206905 non-null object\n",
      "LINENAME                                                                206905 non-null object\n",
      "DIVISION                                                                206905 non-null object\n",
      "DATE                                                                    206905 non-null object\n",
      "TIME                                                                    206905 non-null object\n",
      "DESC                                                                    206905 non-null object\n",
      "ENTRIES                                                                 206905 non-null int64\n",
      "EXITS                                                                   206905 non-null int64\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 17.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Selenium is a powerful tool for controlling web browser through program. It is functional for all browsers, works on all major OS\n",
    "###### Mastering Selenium will help you automate your day to day tasks like controlling your tweets, Whatsapp texting and even just googling without actually opening a browser in just 15-30 lines of python code. The limits of automation is endless with selenium.\n",
    "###### Selenium allows Python to interact with webpages by opening a web browser (e.g. FireFox, Google Chrome, Safari) with either the browser window opening on screen or without the browser window (in a mode called headless)\n",
    "###### Selenium requires a web driver to interface with the chosen browser.Web drivers is a package to interact with web browser. It interacts with the web browser or a remote web server through a wire protocol which is common to all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
